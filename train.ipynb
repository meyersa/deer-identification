{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B9oAcCxV15-J"
      },
      "outputs": [],
      "source": [
        "# importing the zipfile module\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# loading the temp.zip and creating a zip object\n",
        "with ZipFile(\"processed-images.zip\", 'r') as zObject:\n",
        "\n",
        "    # Extracting all the members of the zip\n",
        "    # into a specific location.\n",
        "    zObject.extractall(\n",
        "        path=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vUjf3oShR53U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the CSV data\n",
        "df = pd.read_csv('images.csv', header=None, names=[\"image_path\", \"label\", \"timestamp\", \"moon_phase\", \"wind\", \"weather\", \"temp\", \"humidity\", \"pressure\"])\n",
        "\n",
        "# Filter out rows where image paths do not exist\n",
        "df = df[df['image_path'].apply(os.path.exists)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fg6cg0DR_gY",
        "outputId": "ed36fc92-2ae6-4993-9faf-6bd1007cb8a6"
      },
      "outputs": [],
      "source": [
        "# Convert timestamp to datetime with UTC conversion, with error handling\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce', utc=True)\n",
        "\n",
        "# Drop rows with invalid timestamps (NaT)\n",
        "df = df.dropna()\n",
        "\n",
        "# Extract features from the timestamp\n",
        "df['hour'] = df['timestamp'].dt.hour  # Hour of the day\n",
        "df['day_of_week'] = df['timestamp'].dt.dayofweek  # Day of the week\n",
        "df['is_night'] = (df['hour'] < 6) | (df['hour'] > 18)  # Simple binary night/day indicator\n",
        "\n",
        "# Handle categorical features (moon_phase, weather) with one-hot encoding\n",
        "df = pd.get_dummies(df, columns=['moon_phase', 'weather'], drop_first=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FfIVHiJGSFZ1"
      },
      "outputs": [],
      "source": [
        "# Extract features as a NumPy array\n",
        "metadata_columns = ['hour', 'day_of_week', 'is_night', 'wind', 'temp', 'humidity', 'pressure'] + \\\n",
        "                   [col for col in df.columns if col.startswith('moon_phase_') or col.startswith('weather_')]\n",
        "metadata = df[metadata_columns].values  # Convert to NumPy array\n",
        "metadata = metadata.astype(np.float32)  # Ensure float32 dtype\n",
        "\n",
        "# Convert the label to a NumPy array (binary classification: deer or not-deer)\n",
        "labels = (df['label'] == 'deer').astype(int).values  # Convert to 0 or 1 (0: not deer, 1: deer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q6864GpySK0x"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Image loading function with normalization\n",
        "def load_image(image_path, target_size=(224, 224)):\n",
        "    img = image.load_img(image_path, target_size=target_size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = img_array / 255.0  # Normalize pixel values to [0, 1]\n",
        "    return img_array.astype(np.float32)  # Ensure it's in float32 format\n",
        "\n",
        "# Load all images in parallel using ThreadPoolExecutor\n",
        "def load_images_in_parallel(image_paths, batch_size=32, target_size=(224, 224)):\n",
        "    images = []\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        for i in range(0, len(image_paths), batch_size):\n",
        "            batch_paths = image_paths[i:i + batch_size]\n",
        "            batch_images = list(executor.map(lambda path: load_image(path, target_size), batch_paths))\n",
        "            images.extend(batch_images)\n",
        "    return np.array(images)\n",
        "\n",
        "# Load images and convert to NumPy array\n",
        "images = load_images_in_parallel(df['image_path'].values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdssls3fSNyw",
        "outputId": "f60098ee-24e6-4170-ebfd-aa0161c690a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2129, 224, 224, 3) float32\n",
            "(2129, 16) float32\n",
            "(2129,) int64\n"
          ]
        }
      ],
      "source": [
        "# Ensure correct shape and dtype\n",
        "print(images.shape, images.dtype)  # Should be (num_samples, 224, 224, 3) and float32\n",
        "print(metadata.shape, metadata.dtype)  # Should be (num_samples, N) and float32 (N depends on one-hot encoding)\n",
        "print(labels.shape, labels.dtype)  # Should be (num_samples,) and int32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NjW1vKKSSTkr"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train_images, X_val_images, X_train_metadata, X_val_metadata, y_train, y_val = train_test_split(\n",
        "    images, metadata, labels, test_size=0.2, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KEHFwPUM_q-E"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-19 09:52:02.032868: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
            "2024-11-19 09:52:02.032919: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
            "2024-11-19 09:52:02.032930: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
            "2024-11-19 09:52:02.032948: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-11-19 09:52:02.032972: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers, models, optimizers\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the model with dropout and learning rate scheduler\n",
        "image_input = layers.Input(shape=(224, 224, 3))  # Shape of the images (224x224 RGB images)\n",
        "metadata_input = layers.Input(shape=(metadata.shape[1],))  # Shape of metadata (after one-hot encoding)\n",
        "\n",
        "# Image model\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu')(image_input)\n",
        "x = layers.MaxPooling2D()(x)\n",
        "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "x = layers.MaxPooling2D()(x)\n",
        "x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n",
        "x = layers.MaxPooling2D()(x)\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "# Combine image model and metadata\n",
        "combined = layers.concatenate([x, metadata_input])\n",
        "\n",
        "# Add a fully connected layer, dropout for regularization, and output layer\n",
        "x = layers.Dense(128, activation='relu')(combined)\n",
        "x = layers.Dropout(0.5)(x)  # Dropout to reduce overfitting\n",
        "x = layers.Dense(1, activation='sigmoid')(x)  # Sigmoid for binary classification\n",
        "\n",
        "# Define the model\n",
        "model = models.Model(inputs=[image_input, metadata_input], outputs=x)\n",
        "\n",
        "# Compile the model with a learning rate scheduler\n",
        "initial_lr = 0.001\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_lr, decay_steps=10000, decay_rate=0.9, staircase=True\n",
        ")\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "2GjOJancSa9k",
        "outputId": "7ad9096a-17c8-4868-a677-8a6b70aa997e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/augustmeyers/Coding/deer-identification/.venv/lib/python3.9/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor', 'keras_tensor_1']. Received: the structure of inputs=('*', '*')\n",
            "  warnings.warn(\n",
            "2024-11-19 09:52:03.778965: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 224ms/step - accuracy: 0.6154 - loss: 0.8379 - val_accuracy: 0.6972 - val_loss: 0.5858\n",
            "Epoch 2/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.7505 - loss: 0.5435 - val_accuracy: 0.7277 - val_loss: 0.5605\n",
            "Epoch 3/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.7255 - loss: 0.5443 - val_accuracy: 0.7254 - val_loss: 0.5549\n",
            "Epoch 4/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.7557 - loss: 0.5281 - val_accuracy: 0.7347 - val_loss: 0.5269\n",
            "Epoch 5/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.7631 - loss: 0.5135 - val_accuracy: 0.7535 - val_loss: 0.4853\n",
            "Epoch 6/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.7827 - loss: 0.4724 - val_accuracy: 0.7535 - val_loss: 0.5030\n",
            "Epoch 7/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.7680 - loss: 0.5035 - val_accuracy: 0.7488 - val_loss: 0.4820\n",
            "Epoch 8/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 175ms/step - accuracy: 0.7825 - loss: 0.4798 - val_accuracy: 0.8075 - val_loss: 0.4266\n",
            "Epoch 9/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 180ms/step - accuracy: 0.7931 - loss: 0.4684 - val_accuracy: 0.8122 - val_loss: 0.3903\n",
            "Epoch 10/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 183ms/step - accuracy: 0.8209 - loss: 0.4040 - val_accuracy: 0.8380 - val_loss: 0.3772\n",
            "Epoch 11/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 181ms/step - accuracy: 0.8434 - loss: 0.3675 - val_accuracy: 0.7559 - val_loss: 0.5025\n",
            "Epoch 12/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 174ms/step - accuracy: 0.8058 - loss: 0.4462 - val_accuracy: 0.8216 - val_loss: 0.3938\n",
            "Epoch 13/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 163ms/step - accuracy: 0.8303 - loss: 0.3819 - val_accuracy: 0.8380 - val_loss: 0.3460\n",
            "Epoch 14/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 165ms/step - accuracy: 0.8442 - loss: 0.3428 - val_accuracy: 0.8404 - val_loss: 0.3405\n",
            "Epoch 15/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 162ms/step - accuracy: 0.8574 - loss: 0.3266 - val_accuracy: 0.8545 - val_loss: 0.3126\n",
            "Epoch 16/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 163ms/step - accuracy: 0.8538 - loss: 0.3108 - val_accuracy: 0.8662 - val_loss: 0.3284\n",
            "Epoch 17/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 159ms/step - accuracy: 0.8853 - loss: 0.2720 - val_accuracy: 0.8404 - val_loss: 0.3663\n",
            "Epoch 18/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 163ms/step - accuracy: 0.9051 - loss: 0.2256 - val_accuracy: 0.8685 - val_loss: 0.3085\n",
            "Epoch 19/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 172ms/step - accuracy: 0.9016 - loss: 0.2152 - val_accuracy: 0.8545 - val_loss: 0.3509\n",
            "Epoch 20/20\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 159ms/step - accuracy: 0.8961 - loss: 0.2118 - val_accuracy: 0.8756 - val_loss: 0.3110\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8858 - loss: 0.3073\n",
            "Validation Loss: 0.31103724241256714\n",
            "Validation Accuracy: 0.8755868673324585\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.71      0.81       159\n",
            "           1       0.85      0.97      0.91       267\n",
            "\n",
            "    accuracy                           0.88       426\n",
            "   macro avg       0.90      0.84      0.86       426\n",
            "weighted avg       0.88      0.88      0.87       426\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X_train_images, X_train_metadata],  # Input data (images and metadata)\n",
        "    y_train,  # Labels\n",
        "    validation_data=([X_val_images, X_val_metadata], y_val),  # Validation data\n",
        "    epochs=20,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Model evaluation\n",
        "val_loss, val_acc = model.evaluate([X_val_images, X_val_metadata], y_val)\n",
        "print(f\"Validation Loss: {val_loss}\")\n",
        "print(f\"Validation Accuracy: {val_acc}\")\n",
        "\n",
        "# Predictions and classification report\n",
        "y_pred = (model.predict([X_val_images, X_val_metadata]) > 0.5).astype(int)  # Convert predictions to 0 or 1\n",
        "\n",
        "# Classification report (Precision, Recall, F1-Score)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
